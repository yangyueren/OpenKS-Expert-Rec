{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives examples about how to generate:\n",
    "\n",
    "word_dict.pkl: convert the words in news titles into indexes.\n",
    "\n",
    "word_dict_all.pkl: convert the words in news titles and abstracts into indexes.\n",
    "\n",
    "embedding.npy: pretrained word embedding matrix of words in word_dict.pkl\n",
    "\n",
    "embedding_all.npy: pretrained embedding matrix of words in word_dict_all.pkl\n",
    "\n",
    "vert_dict.pkl: convert news verticals into indexes.\n",
    "\n",
    "subvert_dict.pkl: convert news subverticals into indexes.\n",
    "\n",
    "uid2index.pkl: convert user ids into indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import ujson as json\n",
    "def save_to_disk(obj, file_name):\n",
    "    print('saving ', file_name)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pkl.dump(obj, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    " \n",
    "def load_from_disk(file_name):\n",
    "    print('loading ', file_name)\n",
    "    with open(file_name, 'rb') as f:\n",
    "        obj = pkl.load(f)\n",
    "    return obj\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    \"\"\"Tokenize a sententence\n",
    "\n",
    "    Args:\n",
    "        sent: the sentence need to be tokenized\n",
    "\n",
    "    Returns:\n",
    "        list: words in the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # treat consecutive words or special punctuation as words\n",
    "    sent = sent.replace('\\t', ' ')\n",
    "    sent = sent.replace('<br/>', ' ')\n",
    "    sent = sent.replace('\\n', ' ')\n",
    "    sent = re.sub(' +', ' ', sent)\n",
    "    pat = re.compile(r\"[\\w]+|[.,!?;|]\")\n",
    "    if isinstance(sent, str):\n",
    "        return pat.findall(sent.lower())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '/data2/yyr/dataset/glove.840B.300d.txt'\n",
    "word_embedding_dim = 300\n",
    "paper_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_paper.pkl'\n",
    "project_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_project.pkl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_words(glove_path):\n",
    "    words = set()\n",
    "    with open(glove_path, \"r\") as f:\n",
    "        for line in tqdm(f):  # noqa: E741 ambiguous variable name 'l'\n",
    "            # return l\n",
    "            l = line.split()  # noqa: E741 ambiguous variable name 'l'\n",
    "            word = l[0]\n",
    "            words.add(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_project.pkl\n",
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_paper.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "projects = load_from_disk(project_path)\n",
    "papers = load_from_disk(paper_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [00:58, 37276.16it/s]\n"
     ]
    }
   ],
   "source": [
    "word_cnt = Counter()\n",
    "word_cnt_all = Counter()\n",
    "glove_w = glove_words(glove_path)\n",
    "\n",
    "for p in projects:\n",
    "    p = json.loads(p)\n",
    "    title = p['AwardTitle'] if 'AwardTitle' in p else \"\"\n",
    "    abstract = p['AbstractNarration'] if 'AbstractNarration' in p else \"\"\n",
    "    t = word_tokenize(title)\n",
    "    t = [i for i in t if i in glove_w]\n",
    "    a = word_tokenize(abstract) + t\n",
    "    a = [i for i in a if i in glove_w]\n",
    "\n",
    "    word_cnt.update(t)\n",
    "    word_cnt_all.update(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in papers:\n",
    "    title = p['title'] if 'title' in p else \"\"\n",
    "    abstract = p['abstract'] if 'abstract' in p else \"\"\n",
    "    t = word_tokenize(title)\n",
    "    t = [i for i in t if i in glove_w]\n",
    "    a = word_tokenize(abstract) + t\n",
    "    a = [i for i in a if i in glove_w]\n",
    "    word_cnt.update(t)\n",
    "    word_cnt_all.update(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {k: v+1 for k, v in zip(word_cnt, range(len(word_cnt)))}\n",
    "word_dict_all = {k: v+1 for k, v in zip(word_cnt_all, range(len(word_cnt_all)))}\n",
    "save_to_disk(word_dict, './data/word_dict.pkl')\n",
    "save_to_disk(word_dict_all, './data/word_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成category\n",
    "\n",
    "vert_dict.pkl: convert news verticals into indexes.\n",
    "\n",
    "subvert_dict.pkl: convert news subverticals into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project2topic_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/project2topic.pkl'\n",
    "paper2topic_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/paper2topic.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/project2topic.pkl\n",
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/paper2topic.pkl\n",
      "41\n",
      "saving  ./data/vert_dict.pkl\n",
      "saving  ./data/subvert_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "topic2id = {'null': 0}\n",
    "topicset = set()\n",
    "project2topic = load_from_disk(project2topic_path)\n",
    "for pt in project2topic:\n",
    "    p, t = pt\n",
    "    topicset.add('topic_project' + str(t))\n",
    "paper2topic = load_from_disk(paper2topic_path)\n",
    "for pt in paper2topic:\n",
    "    p, t = pt\n",
    "    topicset.add('topic_paper' + str(t))\n",
    "\n",
    "for t in topicset:\n",
    "    topic2id[t] = len(topic2id)\n",
    "\n",
    "print(len(topic2id))\n",
    "save_to_disk(topic2id, './data/vert_dict.pkl')\n",
    "save_to_disk(topic2id, './data/subvert_dict.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venue = set()\n",
    "# cnt = 0\n",
    "# cnt_raw = 0\n",
    "# cnt_id = 0\n",
    "# cnt_name_d = 0\n",
    "# raw, id = set(), set()\n",
    "# for p in papers:\n",
    "#     paperid = p['_id']\n",
    "#     if 'venue' not in p:\n",
    "#         cnt+=1\n",
    "#     else:\n",
    "#         if 'raw' not in p['venue']:\n",
    "#             cnt_raw += 1\n",
    "#         else:\n",
    "#             raw.add(p['venue']['raw'])\n",
    "\n",
    "# for r in raw:\n",
    "#     topic2id[r] = len(topic2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拿到embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_glove_matrix(glove_path, word_dict, word_embedding_dim):\n",
    "    \"\"\"Load pretrained embedding metrics of words in word_dict\n",
    "\n",
    "    Args:\n",
    "        path_emb (string): Folder path of downloaded glove file\n",
    "        word_dict (dict): word dictionary\n",
    "        word_embedding_dim: dimention of word embedding vectors\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray, list: pretrained word embedding metrics, words can be found in glove files\n",
    "    \"\"\"\n",
    "    print(len(word_dict))\n",
    "    embedding_matrix = np.random.random((len(word_dict) + 1, word_embedding_dim))\n",
    "    exist_word = []\n",
    "\n",
    "    with open(glove_path, \"r\") as f:\n",
    "        for line in tqdm(f):  # noqa: E741 ambiguous variable name 'l'\n",
    "            # return l\n",
    "            l = line.split()  # noqa: E741 ambiguous variable name 'l'\n",
    "            word = l[0]\n",
    "            # assert len(l) == 301, 'error'\n",
    "            if len(l) != 301:\n",
    "                continue\n",
    "            if len(word) != 0:\n",
    "                if word in word_dict:\n",
    "                    try:\n",
    "                        wordvec = [float(x) for x in l[1:]]\n",
    "                    except Exception as e:\n",
    "                        # import pdb;pdb.set_trace()\n",
    "                        pass\n",
    "                        print(line)\n",
    "                        print(len(l))\n",
    "                        print(word)\n",
    "                        return \n",
    "                    index = word_dict[word]\n",
    "                    embedding_matrix[index] = np.array(wordvec)\n",
    "                    exist_word.append(word)\n",
    "\n",
    "    return embedding_matrix, exist_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = load_from_disk('./data/word_dict.pkl')\n",
    "word_dict_all = load_from_disk('./data/word_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '/data2/yyr/dataset/glove.840B.300d.txt'\n",
    "word_embedding_dim = 300\n",
    "# a = load_glove_matrix(glove_path, word_dict, word_embedding_dim)\n",
    "embedding_matrix, exist_word = load_glove_matrix(glove_path, word_dict, word_embedding_dim)\n",
    "embedding_all_matrix, exist_all_word = load_glove_matrix(glove_path, word_dict_all, word_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('./data', 'embedding.npy'), embedding_matrix)\n",
    "np.save(os.path.join('./data', 'embedding_all.npy'), embedding_all_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(exist_word)\n",
    "print(len(exist_word))\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_all_word[200:240]\n",
    "words = [w for w in word_dict_all]\n",
    "words = set(words)\n",
    "exist = set(exist_all_word)\n",
    "notin = list(words - exist)\n",
    "# notin = list(exist)\n",
    "print(len(notin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notindi = {}\n",
    "for i in notin:\n",
    "    notindi[i] = word_cnt_all[i]\n",
    "val = [v for k,v in notindi.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.array(val)\n",
    "np.sum(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uid2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_person.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65500/65500 [00:00<00:00, 996221.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving  ./data/uid2index.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "person_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_person.pkl'\n",
    "person = load_from_disk(person_path)\n",
    "uid2index = {}\n",
    "\n",
    "for l in tqdm(person):\n",
    "    uid = l\n",
    "    if uid not in uid2index:\n",
    "        uid2index[uid] = len(uid2index) + 1\n",
    "save_to_disk(uid2index, './data/uid2index.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成train和val test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74fc14c592dd70078ea57d4a0f90f037cc0e88e13eda8cf5563781d0a535876b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('KeyBench')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
