{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is to generate train and valid datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import ujson as json\n",
    "def save_to_disk(obj, file_name):\n",
    "    print('saving ', file_name)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pkl.dump(obj, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    " \n",
    "def load_from_disk(file_name):\n",
    "    print('loading ', file_name)\n",
    "    with open(file_name, 'rb') as f:\n",
    "        obj = pkl.load(f)\n",
    "    return obj\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    \"\"\"Tokenize a sententence\n",
    "\n",
    "    Args:\n",
    "        sent: the sentence need to be tokenized\n",
    "\n",
    "    Returns:\n",
    "        list: words in the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # treat consecutive words or special punctuation as words\n",
    "    sent = sent.replace('\\t', ' ')\n",
    "    sent = sent.replace('<br/>', ' ')\n",
    "    sent = sent.replace('\\n', ' ')\n",
    "    sent = re.sub(' +', ' ', sent)\n",
    "    pat = re.compile(r\"[\\w]+|[.,!?;|]\")\n",
    "    if isinstance(sent, str):\n",
    "        return pat.findall(sent.lower())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成 news.tsv，用tab切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  ./data/word_dict_all.pkl\n"
     ]
    }
   ],
   "source": [
    "word_dict_all_path = './data/word_dict_all.pkl'\n",
    "word_dict_all = load_from_disk(word_dict_all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_project.pkl\n",
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_paper.pkl\n"
     ]
    }
   ],
   "source": [
    "paper_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_paper.pkl'\n",
    "project_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/entities_project.pkl'\n",
    "projects = load_from_disk(project_path)\n",
    "papers = load_from_disk(paper_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/NER/paper2wiki_0_-1.pkl\n"
     ]
    }
   ],
   "source": [
    "# 获得 实体\n",
    "# paper2wiki_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/NER/paper2wiki_0_-1.pkl'\n",
    "# project2wiki_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/NER/project2wiki_0_-1.pkl'\n",
    "# paper2wiki = load_from_disk(paper2wiki_path)\n",
    "# project2wiki = load_from_disk(project2wiki_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract(wiki):\n",
    "#     itself = []\n",
    "#     for w in wiki:\n",
    "#         tmp = {}\n",
    "#         tmp['Label'] = w['wiki_label']\n",
    "#         tmp['Type'] = 'P'\n",
    "#         tmp['WikidataId'] = w['wiki_id']\n",
    "#         tmp['Confidence'] = 1.0\n",
    "#         tmp['OccurrenceOffsets'] = [w['pos'][0]]\n",
    "#         tmp['SurfaceForms'] = w['text']\n",
    "#         itself.append(tmp)\n",
    "#     return itself\n",
    "# project2wikiy = {}\n",
    "# for p in project2wiki:\n",
    "#     id = p['AwardID']\n",
    "#     title = extract(p['AwardTitle']['wiki'])\n",
    "#     abs = extract(p['AbstractNarration']['wiki'])\n",
    "#     project2wikiy[id] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/project2topic.pkl\n",
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/paper2topic.pkl\n"
     ]
    }
   ],
   "source": [
    "### 加载category\n",
    "news2topic = {}\n",
    "\n",
    "project2topic_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/project2topic.pkl'\n",
    "f = load_from_disk(project2topic_path)\n",
    "\n",
    "for d in f:\n",
    "    news2topic[d[0]] = 'topic_project'+str(d[1])\n",
    "\n",
    "paper2topic_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/LDA/paper2topic.pkl'\n",
    "f = load_from_disk(paper2topic_path)\n",
    "for d in f:\n",
    "    news2topic[d[0]] = 'topic_paper'+str(d[1])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2962215"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news2topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107125/107125 [00:22<00:00, 4841.61it/s]\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "for p in tqdm(projects):\n",
    "    p = json.loads(p)\n",
    "    title = '' if 'AwardTitle' not in p else p['AwardTitle']\n",
    "    abs = '' if 'AbstractNarration' not in p else p['AbstractNarration']\n",
    "    id = p['AwardID']\n",
    "    category = 'null' if id not in news2topic else news2topic[id]\n",
    "    entity_title = []\n",
    "    entity_abs = []\n",
    "    news_url = ''\n",
    "\n",
    "    t = word_tokenize(title)\n",
    "    t = [i for i in t if i in word_dict_all]\n",
    "    title = ' '.join(t)\n",
    "\n",
    "    t = word_tokenize(abs)\n",
    "    t = [i for i in t if i in word_dict_all]\n",
    "    abs = ' '.join(t)\n",
    "\n",
    "    ss = f'{id}\\t{category}\\t{category}\\t{title}\\t{abs}\\t{news_url}\\t{json.dumps(entity_title)}\\t{json.dumps(entity_abs)}'\n",
    "    news.append(ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2855090/2855090 [01:25<00:00, 33461.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for p in tqdm(papers):\n",
    "    if len(p['authors']) == 0:\n",
    "        continue\n",
    "    title = '' if 'title' not in p else p['title']\n",
    "    abs = '' if 'abstract' not in p else p['abstract']\n",
    "    id = p['_id']\n",
    "    category = 'null' if id not in news2topic else news2topic[id]\n",
    "    entity_title = []\n",
    "    entity_abs = []\n",
    "    news_url = ''\n",
    "\n",
    "    t = word_tokenize(title)\n",
    "    t = [i for i in t if i in word_dict_all]\n",
    "    title = ' '.join(t)\n",
    "\n",
    "    t = word_tokenize(abs)\n",
    "    t = [i for i in t if i in word_dict_all]\n",
    "    abs = ' '.join(t)\n",
    "\n",
    "    ss = f'{id}\\t{category}\\t{category}\\t{title}\\t{abs}\\t{news_url}\\t{json.dumps(entity_title)}\\t{json.dumps(entity_abs)}'\n",
    "    news.append(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/news.tsv', 'w') as f:\n",
    "    for new in news:\n",
    "        f.write(new)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成 behaviors.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/rel_is_publisher_of.pkl\n",
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/train_rel_is_principal_investigator_of.pkl\n"
     ]
    }
   ],
   "source": [
    "publish_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/rel_is_publisher_of.pkl'\n",
    "rel_publisher = load_from_disk(publish_path)\n",
    "prin_invstor_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/train_rel_is_principal_investigator_of.pkl'\n",
    "rel_prin_inves = load_from_disk(prin_invstor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60138/60138 [00:02<00:00, 24723.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "year2news = defaultdict(set)\n",
    "\n",
    "person2news = {}\n",
    "for line in rel_publisher:\n",
    "    _, auid, newsid, year = line\n",
    "    if year >= 2015:\n",
    "        continue\n",
    "    if auid not in person2news:\n",
    "        person2news[auid] = defaultdict(set)\n",
    "    person2news[auid][year].add(newsid)\n",
    "    # year2news[year].add(newsid) 不需要做负样本\n",
    "\n",
    "for line in rel_prin_inves:\n",
    "    _, auid, newsid, year, _ = line\n",
    "    if auid not in person2news:\n",
    "        person2news[auid] = defaultdict(set)\n",
    "    person2news[auid][year].add(newsid)\n",
    "    year2news[year].add(newsid)\n",
    "\n",
    "for y in year2news:\n",
    "    year2news[y] = list(year2news[y])\n",
    "\n",
    "behavior = []\n",
    "for person in tqdm(person2news.keys()):\n",
    "    collect = person2news[person]\n",
    "    for year in collect.keys():\n",
    "        if year < 2000:\n",
    "            continue\n",
    "        if year >= 2015:\n",
    "            continue\n",
    "        year_news = collect[year]\n",
    "        neg = set()\n",
    "        cnt = 0\n",
    "        while len(neg) < len(year_news):\n",
    "            neg_c = random.choice(year2news[year])\n",
    "            if neg_c not in year_news:\n",
    "                neg.add(neg_c)\n",
    "            cnt += 1\n",
    "            if cnt > len(year_news) * 2:\n",
    "                break\n",
    "        history = []\n",
    "        for hy in collect:\n",
    "            if hy < year:\n",
    "                history += list(collect[hy])\n",
    "        ss = (person, year, list(history), list(year_news), list(neg))\n",
    "        behavior.append(ss)\n",
    "print(len(behavior))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisc = []\n",
    "impc = []\n",
    "with open('./train/behaviors.tsv', 'w') as f:\n",
    "    random.shuffle(behavior)\n",
    "    for i, ss in enumerate(behavior):\n",
    "        person, year, history, pos, neg = ss\n",
    "        pos = [p+'-1' for p in pos if '-' not in p]\n",
    "        neg = [n+'-0' for n in neg if '-' not in n]\n",
    "        imp = pos + neg\n",
    "        imp = ' '.join(imp)\n",
    "        history = ' '.join(history)\n",
    "        time = f'11/11/{year:04d} 3:28:58 PM'\n",
    "        st = f'{i+1}\\t{person}\\t{time}\\t{history}\\t{imp}\\n'\n",
    "        f.write(st)\n",
    "        hisc.append(len(history.split()))\n",
    "        impc.append(len(imp.split()))\n",
    "print(Counter(hisc))\n",
    "print(Counter(impc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/rel_is_publisher_of.pkl\n",
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/test_rel_is_principal_investigator_of.pkl\n"
     ]
    }
   ],
   "source": [
    "publish_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/rel_is_publisher_of.pkl'\n",
    "rel_publisher = load_from_disk(publish_path)\n",
    "prin_invstor_path = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/test_rel_is_principal_investigator_of.pkl'\n",
    "rel_prin_inves = load_from_disk(prin_invstor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  /home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/train_rel_is_principal_investigator_of.pkl\n",
      "22.27108301357359\n",
      "2223800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "year2news = defaultdict(set)\n",
    "\n",
    "person2cnt = defaultdict(int)\n",
    "person2news = {}\n",
    "prin_invstor_path_train = '/home/zy/data2/yyr/codes/kg4proj-rec/OpenKS-Expert-Rec/data/datav1/train_rel_is_principal_investigator_of.pkl'\n",
    "rel_prin_inves_train = load_from_disk(prin_invstor_path_train)\n",
    "\n",
    "for line in rel_publisher:\n",
    "    _, auid, newsid, year = line\n",
    "    if auid not in person2news:\n",
    "        person2news[auid] = defaultdict(set)\n",
    "    person2news[auid][year].add(newsid)\n",
    "    # year2news[year].add(newsid) 不需要做负样本\n",
    "    person2cnt[auid] += 1\n",
    "val = [v for k,v in person2cnt.items()]\n",
    "print(np.array(val).mean())\n",
    "\n",
    "for line in rel_prin_inves_train:\n",
    "    _, auid, newsid, year, _ = line\n",
    "    if auid not in person2news:\n",
    "        person2news[auid] = defaultdict(set)\n",
    "    person2news[auid][year].add(newsid)\n",
    "    year2news[year].add(newsid)\n",
    "\n",
    "for y in year2news:\n",
    "    year2news[y] = list(year2news[y])\n",
    "\n",
    "\n",
    "\n",
    "behavior = []\n",
    "cnt = 1\n",
    "for line in rel_prin_inves:\n",
    "    _, auid, newsid, year, negs = line\n",
    "    history = []\n",
    "    if auid in person2news:\n",
    "        collect = person2news[auid]\n",
    "        \n",
    "        for y in collect.keys():\n",
    "            if y >= year:\n",
    "                continue\n",
    "            year_news = collect[y]\n",
    "            history += list(year_news)\n",
    "    history = ' '.join(history)\n",
    "    time = f'11/11/{year:04d} 3:28:58 PM'\n",
    "\n",
    "    ss = f'{cnt}\\t{auid}\\t{time}\\t{history}\\t{newsid+\"-1\"}\\n'\n",
    "    behavior.append(ss)\n",
    "    cnt += 1\n",
    "\n",
    "    for neg in negs:\n",
    "        \n",
    "        history = []\n",
    "        if neg in person2news:\n",
    "            collect = person2news[neg]\n",
    "            for y in collect.keys():\n",
    "                if y >= year:\n",
    "                    continue\n",
    "                year_news = collect[y]\n",
    "                history += list(year_news)\n",
    "        history = ' '.join(history)\n",
    "        time = f'11/11/{year:04d} 5:55:55 PM'\n",
    "\n",
    "        ss = f'{cnt}\\t{neg}\\t{time}\\t{history}\\t{newsid+\"-0\"}\\n'\n",
    "        behavior.append(ss)\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "print(len(behavior))\n",
    "\n",
    "with open('./valid/behaviors.tsv', 'w') as f:\n",
    "    for ss in behavior:\n",
    "        f.write(ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74fc14c592dd70078ea57d4a0f90f037cc0e88e13eda8cf5563781d0a535876b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('KeyBench')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
